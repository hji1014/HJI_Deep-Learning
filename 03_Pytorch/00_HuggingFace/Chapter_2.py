""" [Chapter 2 : Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•˜ê¸°]
1) pipeline ë‚´ë¶€ ì‹¤í–‰ ê³¼ì •
2) models
"""


"""
1) Pipeline ë‚´ë¶€ ì‹¤í–‰ ê³¼ì •

<AutoTokenizer>
- pipeline()ëŠ” ì „ì²˜ë¦¬(preprocessing, ex. tokenizer)->ëª¨ë¸ë¡œ ì…ë ¥ ì „ë‹¬->í›„ì²˜ë¦¬(postprocessing) ì´ 3ë‹¨ê³„ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
- ëª¨ë“  ì „ì²˜ë¦¬(preprocessing)ëŠ” ëª¨ë¸ì´ ì‚¬ì „ í•™ìŠµ(pretraining)ë  ë•Œì™€ ì •í™•íˆ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ë¨¼ì € Model Hubì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•¨
- ì´ë¥¼ ìœ„í•´ AutoTokenizer í´ë˜ìŠ¤ì™€ from_pretrained() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•¨
- ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸(checkpoint) ì´ë¦„ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €(tokenizer)ì™€ ì—°ê²°ëœ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì™€ ìºì‹œí•¨
- ì½”ë“œë¥¼ ì²˜ìŒ ì‹¤í–‰í•  ë•Œë§Œ í•´ë‹¹ ì •ë³´ê°€ ë‹¤ìš´ë¡œë“œë¨

<AutoModel>
- í† í¬ë‚˜ì´ì €ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ pretrained modelì„ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆìŒ
- AutoTokenizer í´ë˜ìŠ¤ì™€ ë§ˆì°¬ê°€ì§€ë¡œ from_pretrained() ë©”ì„œë“œê°€ í¬í•¨ëœ AutoModel í´ë˜ìŠ¤ë¥¼ ì œê³µí•¨
- ì¶œë ¥ ë²¡í„°ëŠ” ì„¸ ê°€ì§€ì˜ ì°¨ì›ìœ¼ë¡œ êµ¬ì„±ë¨ -> 1.batch size, 2.sequence length, 3.hidden size
"""
from transformers import pipeline

classifier = pipeline("sentiment-analysis", device=0)
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)

# pipeline ëœ¯ì–´ë³´ê¸°

# <Tokenizer>
# ë‹¨ì¼ ë¬¸ì¥ ë˜ëŠ” ë‹¤ì¤‘ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì € í•¨ìˆ˜ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì¶œë ¥ í…ì„œ ìœ í˜•ì„ ì§€ì •í•  ìˆ˜ ìˆìŒ
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]

inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")      # input_ids, attention_mask í¬í•¨
print(inputs)

# <Model>
# 1) *Model (hidden statesë¥¼ ë¦¬í„´)
# 2) *ForCausalLM
# 3) *ForMaskedLM
# 4) *ForMultipleChoice
# 5) *ForQuestionAnswering
# 6) *ForSequenceClassification
# 7) *ForTokenClassification
# 8) and othersğŸ¤—
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)

outputs = model(**inputs)                   # output shape : (2, 16, 768)
print(outputs.last_hidden_state.shape)

# sequence classification headê°€ í¬í•¨ë˜ì–´ ìˆëŠ” ëª¨ë¸ë§ ë°©ë²• -> ì¦‰ ëª¨ë¸ ëë‹¨ì— classifierê°€ ì—°ê²°ë˜ì–´ ìˆëŠ” ëª¨ë¸ ë§Œë“œëŠ” ë°©ë²•
# ë‘ ê°œì˜ ë¬¸ì¥ê³¼ ë‘ ê°œì˜ ë ˆì´ë¸”ë§Œ ìˆê¸° ë•Œë¬¸ì—, ëª¨ë¸ì—ì„œ ì–»ì€ ê²°ê³¼ì˜ ëª¨ì–‘(shape)ì€ 2 x 2
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)                   # output shape : (2, 2)
print(outputs.logits.shape)

# ì¶œë ¥ í›„ì²˜ë¦¬í•˜ê¸°
print(outputs.logits)       # ìœ„ì—ì„œ ë„ì¶œëœ ì´ ê°’ì€ ë§ˆì§€ë§‰ ê³„ì¸µì—ì„œ ì¶œë ¥ëœ ì •ê·œí™”ë˜ì§€ ì•Šì€ ì›ì‹œ ì ìˆ˜ì¸ 'logits'ì„

import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)       # softmax ì¶”ê°€
print(predictions)

print(model.config.id2label)       # ê° ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë ˆì´ë¸”ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´, model.configì˜ id2label ì†ì„±ê°’ì„ í™•ì¸


"""
2) models(ëª¨ë¸)
- Transformer model ìƒì„±í•˜ê¸°
- BERT ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ìœ„í•´ ê°€ì¥ ë¨¼ì € í•´ì•¼ í•  ì¼ì€ ì„¤ì •(configuration) ê°ì²´ë¥¼ ë¡œë“œí•˜ëŠ” ê²ƒ
- configë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ì„ ìƒì„±í•˜ë©´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ëª¨ë‘ ì´ˆê¸°í™”ë˜ì–´ ìˆìŒ
- from_pretrained() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŒ
- ì•„ë˜ ì˜ˆì‹œì—ì„œëŠ” bert-base-cased ì‹ë³„ìë¥¼ í†µí•´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì˜€ìŒ(BERT ê°œë°œìê°€ ì§ì ‘ í•™ìŠµí•œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸)
- ìë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ê°€ ë‹¤ìš´ë¡œë“œë˜ê³  ìºì‹œë˜ì–´ ìºì‹œ í´ë”ì— ì €ì¥ë¨(default address : ~/.cache/huggingface/hub)
- ì €ì¥ ë©”ì„œë“œ : save_pretrained() -> config.json : ëª¨ë¸ êµ¬ì¡° ê´€ë ¨ ì†ì„±ë“¤, pytorch_model.bin : ê°€ì¤‘ì¹˜ ì €ì¥
"""
from transformers import BertConfig, BertModel

# config(ì„¤ì •)ì„ ë§Œë“­ë‹ˆë‹¤. -> weight and bias ëª¨ë‘ ì´ˆê¸°í™”
config = BertConfig()

# í•´ë‹¹ configì—ì„œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
model = BertModel(config)

print(config)       # hidden_size : hidden_states vector size, num_hidden_layers : transformer modelì˜ layers ìˆ˜

# ì‚¬ì „í•™ìŠµëœ Bert model ìƒì„±
model = BertModel.from_pretrained("bert-base-cased")

# ëª¨ë¸ ì €ì¥
model.save_pretrained("./bertmodel")

# ì¶”ë¡ 
sequences = ["Hello!", "Cool.", "Nice!"]

encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]

import torch
model_inputs = torch.tensor(encoded_sequences)

output = model(model_inputs)