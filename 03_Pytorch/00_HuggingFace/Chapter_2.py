""" [Chapter 2 : Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•˜ê¸°]
1) pipeline ë‚´ë¶€ ì‹¤í–‰ ê³¼ì •
2) models
3) tokenizer
4) handling multiple sequences(ë‹¤ì¤‘ ì‹œí€€ìŠ¤ ì²˜ë¦¬)
"""


"""
1) Pipeline ë‚´ë¶€ ì‹¤í–‰ ê³¼ì •

<AutoTokenizer>
- pipeline()ëŠ” ì „ì²˜ë¦¬(preprocessing, ex. tokenizer)->ëª¨ë¸ë¡œ ì…ë ¥ ì „ë‹¬->í›„ì²˜ë¦¬(postprocessing) ì´ 3ë‹¨ê³„ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
- ëª¨ë“  ì „ì²˜ë¦¬(preprocessing)ëŠ” ëª¨ë¸ì´ ì‚¬ì „ í•™ìŠµ(pretraining)ë  ë•Œì™€ ì •í™•íˆ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ë¨¼ì € Model Hubì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•¨
- ì´ë¥¼ ìœ„í•´ AutoTokenizer í´ë˜ìŠ¤ì™€ from_pretrained() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•¨
- ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸(checkpoint) ì´ë¦„ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €(tokenizer)ì™€ ì—°ê²°ëœ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì™€ ìºì‹œí•¨
- ì½”ë“œë¥¼ ì²˜ìŒ ì‹¤í–‰í•  ë•Œë§Œ í•´ë‹¹ ì •ë³´ê°€ ë‹¤ìš´ë¡œë“œë¨

<AutoModel>
- í† í¬ë‚˜ì´ì €ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ pretrained modelì„ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆìŒ
- AutoTokenizer í´ë˜ìŠ¤ì™€ ë§ˆì°¬ê°€ì§€ë¡œ from_pretrained() ë©”ì„œë“œê°€ í¬í•¨ëœ AutoModel í´ë˜ìŠ¤ë¥¼ ì œê³µí•¨
- ì¶œë ¥ ë²¡í„°ëŠ” ì„¸ ê°€ì§€ì˜ ì°¨ì›ìœ¼ë¡œ êµ¬ì„±ë¨ -> 1.batch size, 2.sequence length, 3.hidden size
"""
from transformers import pipeline

classifier = pipeline("sentiment-analysis", device=0)
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)

# pipeline ëœ¯ì–´ë³´ê¸°

# <Tokenizer>
# ë‹¨ì¼ ë¬¸ì¥ ë˜ëŠ” ë‹¤ì¤‘ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì € í•¨ìˆ˜ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì¶œë ¥ í…ì„œ ìœ í˜•ì„ ì§€ì •í•  ìˆ˜ ìˆìŒ
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]

inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")      # input_ids, attention_mask í¬í•¨
print(inputs)

# <Model>
# 1) *Model (hidden statesë¥¼ ë¦¬í„´)
# 2) *ForCausalLM
# 3) *ForMaskedLM
# 4) *ForMultipleChoice
# 5) *ForQuestionAnswering
# 6) *ForSequenceClassification
# 7) *ForTokenClassification
# 8) and othersğŸ¤—
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)

outputs = model(**inputs)                   # output shape : (2, 16, 768)
print(outputs.last_hidden_state.shape)

# sequence classification headê°€ í¬í•¨ë˜ì–´ ìˆëŠ” ëª¨ë¸ë§ ë°©ë²• -> ì¦‰ ëª¨ë¸ ëë‹¨ì— classifierê°€ ì—°ê²°ë˜ì–´ ìˆëŠ” ëª¨ë¸ ë§Œë“œëŠ” ë°©ë²•
# ë‘ ê°œì˜ ë¬¸ì¥ê³¼ ë‘ ê°œì˜ ë ˆì´ë¸”ë§Œ ìˆê¸° ë•Œë¬¸ì—, ëª¨ë¸ì—ì„œ ì–»ì€ ê²°ê³¼ì˜ ëª¨ì–‘(shape)ì€ 2 x 2
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)                   # output shape : (2, 2)
print(outputs.logits.shape)

# ì¶œë ¥ í›„ì²˜ë¦¬í•˜ê¸°
print(outputs.logits)       # ìœ„ì—ì„œ ë„ì¶œëœ ì´ ê°’ì€ ë§ˆì§€ë§‰ ê³„ì¸µì—ì„œ ì¶œë ¥ëœ ì •ê·œí™”ë˜ì§€ ì•Šì€ ì›ì‹œ ì ìˆ˜ì¸ 'logits'ì„

import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)       # softmax ì¶”ê°€
print(predictions)

print(model.config.id2label)       # ê° ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë ˆì´ë¸”ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´, model.configì˜ id2label ì†ì„±ê°’ì„ í™•ì¸


"""
2) models(ëª¨ë¸)
- Transformer model ìƒì„±í•˜ê¸°
- BERT ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê¸° ìœ„í•´ ê°€ì¥ ë¨¼ì € í•´ì•¼ í•  ì¼ì€ ì„¤ì •(configuration) ê°ì²´ë¥¼ ë¡œë“œí•˜ëŠ” ê²ƒ
- configë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ì„ ìƒì„±í•˜ë©´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ëª¨ë‘ ì´ˆê¸°í™”ë˜ì–´ ìˆìŒ
- from_pretrained() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŒ
- ì•„ë˜ ì˜ˆì‹œì—ì„œëŠ” bert-base-cased ì‹ë³„ìë¥¼ í†µí•´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì˜€ìŒ(BERT ê°œë°œìê°€ ì§ì ‘ í•™ìŠµí•œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸)
- ìë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ê°€ ë‹¤ìš´ë¡œë“œë˜ê³  ìºì‹œë˜ì–´ ìºì‹œ í´ë”ì— ì €ì¥ë¨(default address : ~/.cache/huggingface/hub)
- ì €ì¥ ë©”ì„œë“œ : save_pretrained() -> config.json : ëª¨ë¸ êµ¬ì¡° ê´€ë ¨ ì†ì„±ë“¤, pytorch_model.bin : ê°€ì¤‘ì¹˜ ì €ì¥
"""
from transformers import BertConfig, BertModel

# config(ì„¤ì •)ì„ ë§Œë“­ë‹ˆë‹¤. -> weight and bias ëª¨ë‘ ì´ˆê¸°í™”
config = BertConfig()

# í•´ë‹¹ configì—ì„œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
model = BertModel(config)

print(config)       # hidden_size : hidden_states vector size, num_hidden_layers : transformer modelì˜ layers ìˆ˜

# ì‚¬ì „í•™ìŠµëœ Bert model ìƒì„±
model = BertModel.from_pretrained("bert-base-cased")

# ëª¨ë¸ ì €ì¥
model.save_pretrained("./bertmodel")

# ì¶”ë¡ 
sequences = ["Hello!", "Cool.", "Nice!"]

encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]

import torch
model_inputs = torch.tensor(encoded_sequences)

output = model(model_inputs)


"""
3) tokenizer(í† í¬ë‚˜ì´ì €)
- ë‹¨ì–´ ê¸°ë°˜ í† í°í™” (Word-based Tokenization)
    - split on spaces
    - split on punctuation
    - ëª¨ë“  ë‹¨ì–´ë¥¼ ê°ë‹¹í• ë ¤ë©´ ì—„ì²­ë‚˜ê²Œ ë§ì€ ë©”ëª¨ë¦¬ê°€ ì†Œë¹„ë¨
    - ì¤‘ìš” ë‹¨ì–´ë§Œ ì¶”ê°€í•˜ë©´ out-of-vocabulary (OOV, unknown) ë¬¸ì œì— ì§ë©´í•˜ê²Œ ë¨
- ë¬¸ì ê¸°ë°˜ í† í°í™” (Character-based Tokenization)
- í•˜ìœ„ ë‹¨ì–´ í† í°í™” (Subword Tokenization)
    - ìœ„ì˜ ë‘ ê°€ì§€ ë°©ë²•ì„ ëª¨ë‘ ì¢…í•©í•œ ë°©ë²•
    - ex) annoyingly -> annoying + ly
- ì„¸ë¶€ ê¸°ë²•ë“¤
    - Byte-level BPE (GPT-2ì— ì‚¬ìš©ë¨)
    - WordPiece (BERTì— ì‚¬ìš©ë¨)
    - SentencePiece, Unigram (ëª‡ëª‡ ë‹¤êµ­ì–´ ëª¨ë¸ì— ì‚¬ìš©ë¨)
    
- í† í¬ë‚˜ì´ì € ë¡œë”© ë° ì €ì¥ : 'ëª¨ë¸ ë¡œë“œ ë° ì €ì¥'ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ from_pretrained()ì™€ save_pretrained() ë©”ì„œë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©

- Encoding : í† í°í™” + ì…ë ¥ ì‹ë³„ì(input IDs)ë¡œì˜ ë³€í™˜ -> 2ë‹¨ê³„
- Decoding : ë‹¤ì‹œ textë¡œ ë³€í™˜í•˜ëŠ” ì¸ì½”ë”© ë°˜ëŒ€ ê³¼ì •
"""

# í† í¬ë‚˜ì´ì € ë¡œë”©(1)
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
tokenizer("Using a Transformer network is simple")

# í† í¬ë‚˜ì´ì € ë¡œë”©(2)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenizer("Using a Transformer network is simple")

# í† í¬ë‚˜ì´ì € ì €ì¥
tokenizer.save_pretrained("./bert_tok")

# í† í°í™” ì‘ì—…(ì¸ì½”ë”©)
sequence = "Using a Transformer network is simple"

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokens = tokenizer.tokenize(sequence)
print(tokens)

# í† í°ì„ input IDsë¡œ ë³€í™˜(ì¸ì½”ë”©)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)

# ë””ì½”ë”©
decoded_string = tokenizer.decode([7993, 170, 13809, 23763, 2443, 1110, 3014])
print(decoded_string)

"""
4) handling multiple sequences(ë‹¤ì¤‘ ì‹œí€€ìŠ¤ ì²˜ë¦¬)
- modelì€ ì…ë ¥ì˜ batch í˜•íƒœë¥¼ ìš”êµ¬í•¨ -> 1ê°œ ë°ì´í„° shape : (1, 14), 2ê°œ ë°ì´í„° shape : (2, 14)
- ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ íŒ¨ë”©í•´ì¤˜ì•¼ í•¨. -> íŒ¨ë”© í† í° ì‹ë³„ì(ID)ëŠ” tokenizer.pad_token_idì— ì €ì¥ë˜ì–´ ìˆìŒ
    -> ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ sequence lengthë¥¼ 512ê°œ ë˜ëŠ” 1024ê°œê¹Œì§€ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
    -> ë” ê¸´ sequenceë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ë¥¸ ëª¨ë¸ì„ ì“°ë˜ê°€, sequenceë¥¼ ì˜ë¼ì„œ ë„£ì–´ì•¼ í•¨(sequence = sequence[:max_sequence_length])
    -> ì ˆë‹¨í•˜ëŠ” ê²ƒì„ truncationì´ë¼ê³  í•¨
"""

# ì—ëŸ¬ ì½”ë“œ -> (14,) í¬ê¸°ì˜ í…ìŠ¤íŠ¸ í•˜ë‚˜ë§Œ ë‹¬ë‘ ë„£ì—ˆì„ ë•Œ -> ì—ëŸ¬ ë°œìƒ
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)

model(input_ids)        # This line will fail

tokenized_inputs = tokenizer(sequence, return_tensors="pt")         # ì´ë ‡ê²Œ í•´ì•¼ í•¨
print(tokenized_inputs["input_ids"])

# ì œëŒ€ë¡œ ì…ë ¥ ë„£ëŠ” ë°©ë²•
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])         # ì‹œí€€ìŠ¤ 1ê°œ
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)

batched_ids = [ids, ids]                # ì‹œí€€ìŠ¤ 2ê°œ
input_ids = torch.tensor(batched_ids)

output = model(input_ids)
print("Logits:", output.logits)

# ì…ë ¥ì„ padding í•˜ê¸°(padding ì „) -> ì°¨ì›ì´ ë§ì§€ ì•Šì•„ tensorë¡œ ë³€í™˜ ì•ˆë¨
batched_ids = [
    [200, 200, 200],
    [200, 200],
]

# ì…ë ¥ì„ padding í•˜ê¸°(padding token : 100)
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]

# íŒ¨ë”©í•œ ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ì— ë„£ì–´ë³´ê¸°
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)          # sequence2ì™€ batched_idsì˜ ë‘ ë²ˆì§¸ ë°ì´í„°ë¡œë¶€í„° ì¶”ë¡ ëœ ê²°ê³¼ê°€ ë‹¤ë¦„ -> pad í† í°ì˜ ì˜í–¥(with attention layers) -> attention maskë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ë”© í† í°ì„ ë¬´ì‹œí•˜ê²Œ ë§Œë“¤ì–´ì•¼ í•¨

batch_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]
outputs = model(torch.tensor(batch_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)       # attention masë¥¼ ì‚¬ìš©í•˜ë©´ sequence2ì™€ ê°™ì€ ê²°ê³¼ê°€ ì¶”ë¡ ë¨

